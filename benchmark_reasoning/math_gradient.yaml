# Math Gradient Configuration
# Tests mathematical reasoning across difficulty levels
# Shows where a model's math capabilities start to break down
# Good for: Understanding math skill ceiling, comparing math performance

global_options:
  random_seed: 42
  output_format: "json"

datasets:
  # Elementary math - should be very solvable
  gsm8k:
    path: "openai/gsm8k"
    format: "huggingface"
    config: "main"
    split: "test"
    mode: "counts"
    difficulty_source: "estimated"
    counts:
      easy: 5      # Grade school level math
      medium: 3    # More complex word problems

  # Advanced undergraduate/graduate level math
  numina_math:
    path: "AI-MO/NuminaMath-TIR"
    format: "huggingface"
    split: "test"
    mode: "counts"
    difficulty_source: "assumed"
    counts:
      hard: 3      # University-level mathematics

  # Competition-level math olympiad problems
  aime_2024:
    path: "Maxwell-Jia/AIME_2024"
    format: "huggingface"
    split: "train"
    mode: "counts"
    difficulty_source: "inherent"
    counts:
      very_hard: 4   # Mathematical olympiad problems

# Total: 15 problems across difficulty spectrum
# Expected performance gradient: High on GSM8K → Medium on Numina → Low on AIME
# Suggested timeout: 120-180 seconds (math reasoning needs time)
# Note: Only numina_math shows "not evaluated" - GSM8K and AIME have full evaluation