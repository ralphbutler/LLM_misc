BitNetTransformer(
  (emb): Embedding(256, 512)
  (transformer): Transformer(
    (layers): ModuleList(
      (0-7): 8 x BitMGQA(
        (q_proj): BitLinear15b(in_features=512, out_features=512, quantization=ternary)
        (k_proj): BitLinear15b(in_features=512, out_features=256, quantization=ternary)
        (v_proj): BitLinear15b(in_features=512, out_features=256, quantization=ternary)
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (out_proj): BitLinear15b(in_features=256, out_features=512, quantization=ternary)
      )
    )
    (ffn_layers): ModuleList(
      (0-7): 8 x BitFeedForward(
        (ff): Sequential(
          (0): Sequential(
            (0): BitLinear15b(in_features=512, out_features=2048, quantization=ternary)
            (1): SiLU()
          )
          (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.1, inplace=False)
          (3): BitLinear15b(in_features=2048, out_features=512, quantization=ternary)
        )
      )
    )
  )
  (to_logits): Sequential(
    (0): RMSNorm()
    (1): Linear(in_features=512, out_features=256, bias=True)
  )
)
--------------------------------------------------
AutoregressiveWrapper(
  (net): BitNetTransformer(
    (emb): Embedding(256, 512)
    (transformer): Transformer(
      (layers): ModuleList(
        (0-7): 8 x BitMGQA(
          (q_proj): BitLinear15b(in_features=512, out_features=512, quantization=ternary)
          (k_proj): BitLinear15b(in_features=512, out_features=256, quantization=ternary)
          (v_proj): BitLinear15b(in_features=512, out_features=256, quantization=ternary)
          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (out_proj): BitLinear15b(in_features=256, out_features=512, quantization=ternary)
        )
      )
      (ffn_layers): ModuleList(
        (0-7): 8 x BitFeedForward(
          (ff): Sequential(
            (0): Sequential(
              (0): BitLinear15b(in_features=512, out_features=2048, quantization=ternary)
              (1): SiLU()
            )
            (1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
            (2): Dropout(p=0.1, inplace=False)
            (3): BitLinear15b(in_features=2048, out_features=512, quantization=ternary)
          )
        )
      )
    )
    (to_logits): Sequential(
      (0): RMSNorm()
      (1): Linear(in_features=512, out_features=256, bias=True)
    )
  )
)
--------------------------------------------------
